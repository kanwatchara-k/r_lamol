# Rational LAMOL

Lifelong learning (LL) aims to train a neural network on a stream of tasks while retaining knowledge from previous tasks. However, many prior attempts in NLP still suffer from the catastrophic forgetting issue, where the model completely forgets what it just learned in the previous tasks.
In this paper, we introduce Rational LAMOL, a novel end-to-end LL framework for language models. In order to alleviate catastrophic forgetting, Rational LAMOL enhances LAMOL, a recent LL model, by applying critical freezing guided by human rationales. When the human rationales are not available, we propose exploiting unsupervised generated rationales as substitutions.  
 
** code mostly taken from LAMOL[https://github.com/jojotenya/LAMOL] **

## Dataset


TODO: \
Write Proper Readme \
Upload Code \
Upload SciFact dataset used in the paper
